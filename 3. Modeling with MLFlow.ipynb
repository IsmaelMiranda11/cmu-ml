{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traning Spaceship-Titanic model.\n",
    "\n",
    "This notebook will use MLflow to track the model training process.\n",
    "\n",
    "## MLflow\n",
    "\n",
    "Install MLflow with pip:\n",
    "\n",
    "```bash\n",
    "pip install mlflow\n",
    "```\n",
    "\n",
    "Run the following command in the **root directory of the project** to start the MLflow server:\n",
    "\n",
    "```bash\n",
    "mlflow server --port 8000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Note: the `train.csv` is the unique dataset that has the target column (**Transported**).\n",
    "\n",
    "Because of that, we will split the data into train and test datasets. The test dataset will be used to evaluate the model after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('data/spaceshit-titanic/train.csv')\n",
    "\n",
    "X, y = data.drop('Transported', axis=1), data['Transported']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                    random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline   import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "## 1. Columns separation\n",
    "# Categorical columns\n",
    "categorical_columns = [\n",
    "    'Group', # it will be created by the passenger_id_spliter\n",
    "    'Deck', # it will be created by the cabin_spliter\n",
    "    'Num', # it will be created by the cabin_spliter\n",
    "    'Side', # it will be created by the cabin_spliter\n",
    "    'VIP',\n",
    "    'CryoSleep',\n",
    "    'HomePlanet',\n",
    "    'Destination'\n",
    "]\n",
    "# Categorical columns to encode. They have low cardinality\n",
    "columns_to_encode = [\n",
    "    'Deck', # it will be created by the cabin_spliter\n",
    "    'Side', # it will be created by the cabin_spliter\n",
    "    'VIP',\n",
    "    'CryoSleep',\n",
    "    'HomePlanet',\n",
    "    'Destination'\n",
    "]\n",
    "\n",
    "# Numeric columns\n",
    "numeric_columns = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "## 2. FunctionTransformers. Custom transformers for columns\n",
    "# Function\n",
    "def passenger_id_spliter(passenger_id_col: pd.Series) -> pd.DataFrame:\n",
    "    '''Function to split the passenger id into two columns\n",
    "\n",
    "    Args:\n",
    "        passenger_id_col: pd.Series - The passenger id column\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame - The dataframe with the two columns (Group, Number)\n",
    "    '''\n",
    "    df = (\n",
    "        pd.DataFrame(\n",
    "            passenger_id_col\n",
    "            .str\n",
    "            .split('_')\n",
    "            .to_list(),\n",
    "        columns=['Group', 'Number'],\n",
    "        index=passenger_id_col.index\n",
    "        )\n",
    "        .drop('Number', axis=1)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def cabin_spliter(cabin_col: pd.Series) -> pd.DataFrame:\n",
    "    '''Function to split the cabin into two columns\n",
    "\n",
    "    Args:\n",
    "        cabin_col: pd.Series - The cabin column\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame - The dataframe with three columns (Deck, Num, Side)\n",
    "    '''\n",
    "    df = (\n",
    "        cabin_col\n",
    "        .str\n",
    "        .split('/', expand=True)\n",
    "        .rename(columns={0: 'Deck', 1: 'Num', 2: 'Side'})\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "## 3. Transformer instantiation\n",
    "\n",
    "# Custom transformers\n",
    "passenger_id_transformer = FunctionTransformer(func=passenger_id_spliter,\n",
    "    feature_names_out=lambda _, __: np.array(['Group'])\n",
    ")\n",
    "\n",
    "cabin_transformer = FunctionTransformer(func=cabin_spliter,\n",
    "    feature_names_out=lambda _, __: np.array(['Deck', 'Num', 'Side'])\n",
    ")\n",
    "\n",
    "# Encoders\n",
    "encoder = OrdinalEncoder()\n",
    "one_hot = OneHotEncoder()\n",
    "\n",
    "# Scalers\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Imputer\n",
    "knn = KNNImputer(n_neighbors=10, weights='uniform')\n",
    "simple_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Rounder for the categorical columns\n",
    "rounder = FunctionTransformer(func=lambda x: np.round(x, 0),\n",
    "    feature_names_out=lambda _, __: categorical_columns\n",
    ")\n",
    "\n",
    "## 4. Transformers pipeline\n",
    "\n",
    "# Columns splitter function transformer\n",
    "splitter_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('passenger_id', passenger_id_transformer, 'PassengerId'),\n",
    "        ('cabin', cabin_transformer, 'Cabin')\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    force_int_remainder_cols=False, #type: ignore\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "# Enconder the categorical columns\n",
    "encoder_transfomer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('encoder', encoder, columns_to_encode)\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    force_int_remainder_cols=False, #type: ignore\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "# Imputers and dropping high cardinality columns\n",
    "imputer_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('numeric_imputer', simple_imputer, numeric_columns),\n",
    "        ('categorical_imputer', knn, categorical_columns)\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    force_int_remainder_cols=False, #type: ignore\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "# Rounder and scalers\n",
    "rounder_scaler_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('rounder', rounder, categorical_columns),\n",
    "        ('scaler', scaler, numeric_columns)\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    force_int_remainder_cols=False, #type: ignore\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "# Dropping columns\n",
    "drop_columns_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('drop', 'drop', 'Name')\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    force_int_remainder_cols=False, #type: ignore\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "# Gather all transformers into a pipeline\n",
    "transfomers = Pipeline([\n",
    "    ('splitter', splitter_transformer),\n",
    "    ('encoder', encoder_transfomer),\n",
    "    ('imputer', imputer_transformer),\n",
    "    ('rounder_scaler', rounder_scaler_transformer),\n",
    "    ('drop_columns', drop_columns_transformer)\n",
    "])\n",
    "\n",
    "# Garantee that the output of \"transform\" and \"fit_transform\" will be a pandas dataframe\n",
    "# This is necessary due use label columns into transformers\n",
    "transfomers.set_output(transform='pandas')\n",
    "\n",
    "# Estimator to predict.\n",
    "# They can be chaged in the clf pipeline estimator step\n",
    "nn = MLPClassifier(hidden_layer_sizes=(50, 5), activation='relu', max_iter=1000, random_state=42)\n",
    "logistic = LogisticRegression(random_state=42)\n",
    "random_forest = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Pipeline\n",
    "clf = Pipeline([\n",
    "    ('estimator', nn)\n",
    "])\n",
    "\n",
    "# 5. Final pipeline\n",
    "pipe = Pipeline([\n",
    "    ('transformers', transfomers),\n",
    "    ('clf', clf)\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running experiments in MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the following cells, make sure the MLflow server is running.\n",
    "\n",
    "Run the command below in the root directory of the project:\n",
    "\n",
    "```bash\n",
    "mlflow server --port 8000\n",
    "```\n",
    "\n",
    "Access the MLflow UI by navigating to http://localhost:8000 in your web browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import mlflow\n",
    "\n",
    "# Set the MLflow server\n",
    "mlflow.set_tracking_uri('http://localhost:8000')\n",
    "\n",
    "# Set the experiment, or create and set if not exists\n",
    "mlflow.set_experiment('spaceshit-titanic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "# Run in mlflow\n",
    "with mlflow.start_run(run_name=f\"spaceshit-titanic-{label}\"):\n",
    "\n",
    "    mlflow.set_tag('Project', 'spaceshit-titanic')\n",
    "\n",
    "    # Logar os parametros\n",
    "    params = pipe.get_params(deep=True)\n",
    "    params.pop('memory', {})\n",
    "\n",
    "    mlflow.log_params(params)\n",
    "    mlflow\n",
    "\n",
    "    # Pipeline fit\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    # Export the html of the pipeline\n",
    "    artifacts_temp = 'artifacts_temp'\n",
    "\n",
    "    with open(f'{artifacts_temp}/pipeline.html', 'w', encoding='utf8') as file:\n",
    "        file.write(pipe._repr_html_()) #type: ignore\n",
    "    \n",
    "    mlflow.log_artifact(f'{artifacts_temp}/pipeline.html', 'pipeline')\n",
    "\n",
    "\n",
    "    mlflow.sklearn.log_model(sk_model=pipe,\n",
    "                             artifact_path='spaceshit-titanic-model',\n",
    "                            # registered_model_name=f'spaceshit-titanic'\n",
    "    )\n",
    "\n",
    "    # Metrics\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, pipe.predict_proba(X_test)[:, 1])\n",
    "\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.savefig(f'{artifacts_temp}/roc_curve.png')\n",
    "\n",
    "    mlflow.log_artifact(f'{artifacts_temp}/roc_curve.png', 'metrics')\n",
    "    \n",
    "    roc_auc_score_ = roc_auc_score(y_test, pipe.predict_proba(X_test)[:, 1])\n",
    "\n",
    "    mlflow.log_metric('roc_auc', roc_auc_score_)\n",
    "\n",
    "    cv_accuracy = cross_val_score(pipe, X_train, y_train, cv=10, scoring='accuracy')\n",
    "    mlflow.log_metric('cross_val_accuracy_mean', cv_accuracy.mean())\n",
    "    cv_recall = cross_val_score(pipe, X_train, y_train, cv=10, scoring='recall')\n",
    "    mlflow.log_metric('cross_val_recall_mean', cv_recall.mean())\n",
    "    cv_precision = cross_val_score(pipe, X_train, y_train, cv=10, scoring='precision')\n",
    "    mlflow.log_metric('cross_val_precision', cv_precision.mean())\n",
    "\n",
    "    \n",
    "    # # Log the dataset\n",
    "    # X.to_csv('artifacts_temp/X.json', index=False)\n",
    "    # mlflow.log_artifact('artifacts_temp/X.csv', 'spaceshit-titanic-data')\n",
    "\n",
    "    # dataset: PandasDataset = mlflow.data.from_pandas(X, source='artifacts_temp/X.csv')\n",
    "    # mlflow.log_input(dataset, context='train')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
